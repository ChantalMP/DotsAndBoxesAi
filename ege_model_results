mm500_hsmin128_hsmax1024_nvr-5_lr0.01_d0.5_hl7.h5
Ai Wins: 46, with 565 fields and 20 random moves
 Random Wins: 3 with 112 fields
Epoch 19250 | Loss 0.6586

epsilon 2 gegen 1


https://keras.io/losses/
mean_squared_error vs squared_hinge vs categorical_crossentropy
8 by 8

epsilon 1 mse
    -
epsilon 2 mse
    -
epsilon 1 sh
    -1000 epochs fields = 30 wins 0.5

epsilon 2 sh
    -
epsilon 1 cc
    -did not run
epsilon 2 cc
    -did not run



1.mm500_hsmin512_hsmax1024_lr0.01_d0.5_hl3_na144_epsi0.1_mse.h5
    -Fields 45.6 Wins 93.6 Epoch 6000
2.mm500_hsmin512_hsmax1024_lr0.05_d0.5_hl3_na144_epsi0.1_mse.h5
    -
3.mm500_hsmin512_hsmax1024_lr0.05_d0.5_hl3_na144_epsi0.2_mse.h5
    - Fields 35.6 Wins 59 Epoch 3000
4.mm500_hsmin512_hsmax1024_lr0.01_d0.5_hl3_na144_epsi0.1_sh.h5
    -


https://keras.io/activations/


mm500_hsmin288_hsmax576_lr1.0_d0.5_hl3_na144_tiFalseadadelta.h5
mm500_hsmin288_hsmax576_lr0.1_d0.5_hl3_na144_tiFalsesgd_momentum.h5(momentum=0.8 , decay_rate = learning_rate/epochs)
have both learned to build squares and only take at the end


different_Reward(#different reward if the oppenents turn is next, your turn: you win best reward, else opponent does
                if new_fields == 0:
                    targets[i, action_t] = reward_t - self.discount * Q_sa
                else:
                    targets[i, action_t] = reward_t + self.discount * Q_sa
                    )
                    did not work as expected. It learned to give fields to the oppenent.